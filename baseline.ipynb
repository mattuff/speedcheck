{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3225,"status":"ok","timestamp":1657516309643,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"22IYkFG9GWNU"},"outputs":[],"source":["import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from google.colab import drive\n","\n","import numpy as np\n","import pandas as pd\n","\n","import os\n","import time\n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2549620,"status":"ok","timestamp":1657518859259,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"b7R6_wUiWX86","outputId":"a7d064e2-6b96-471b-bb71-74db1409d03b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["drive.mount('/content/drive/')\n","!unzip -q drive/MyDrive/data/data3.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1657518859260,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"qfxz0StraCEf"},"outputs":[],"source":["# https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py\n","\n","\n","class ConvLSTMCell(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n","        \"\"\"\n","        Initialize ConvLSTM cell.\n","        Parameters\n","        ----------\n","        input_dim: int\n","            Number of channels of input tensor.\n","        hidden_dim: int\n","            Number of channels of hidden state.\n","        kernel_size: (int, int)\n","            Size of the convolutional kernel.\n","        bias: bool\n","            Whether or not to add the bias.\n","        \"\"\"\n","\n","        super(ConvLSTMCell, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.kernel_size = kernel_size\n","        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n","        self.bias = bias\n","\n","        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n","                              out_channels=4 * self.hidden_dim,\n","                              kernel_size=self.kernel_size,\n","                              padding=self.padding,\n","                              bias=self.bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        combined = T.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n","\n","        combined_conv = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = T.split(combined_conv, self.hidden_dim, dim=1)\n","        i = T.sigmoid(cc_i)\n","        f = T.sigmoid(cc_f)\n","        o = T.sigmoid(cc_o)\n","        g = T.tanh(cc_g)\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * T.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, image_size):\n","        height, width = image_size\n","        return (T.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n","                T.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n","\n","\n","class ConvLSTM(nn.Module):\n","\n","    \"\"\"\n","    Parameters:\n","        input_dim: Number of channels in input\n","        hidden_dim: Number of hidden channels\n","        kernel_size: Size of kernel in convolutions\n","        num_layers: Number of LSTM layers stacked on each other\n","        batch_first: Whether or not dimension 0 is the batch or not\n","        bias: Bias or no bias in Convolution\n","        return_all_layers: Return the list of computations for all layers\n","        Note: Will do same padding.\n","    Input:\n","        A tensor of size B, T, C, H, W or T, B, C, H, W\n","    Output:\n","        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n","            0 - layer_output_list is the list of lists of length T of each output\n","            1 - last_state_list is the list of last states\n","                    each element of the list is a tuple (h, c) for hidden state and memory\n","    Example:\n","        \u003e\u003e x = T.rand((32, 10, 64, 128, 128))\n","        \u003e\u003e convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n","        \u003e\u003e _, last_states = convlstm(x)\n","        \u003e\u003e h = last_states[0][0]  # 0 for layer index, 0 for h index\n","    \"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n","                 batch_first=False, bias=True, return_all_layers=False):\n","        super(ConvLSTM, self).__init__()\n","\n","        self._check_kernel_size_consistency(kernel_size)\n","\n","        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n","        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n","        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n","        if not len(kernel_size) == len(hidden_dim) == num_layers:\n","            raise ValueError('Inconsistent list length.')\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.kernel_size = kernel_size\n","        self.num_layers = num_layers\n","        self.batch_first = batch_first\n","        self.bias = bias\n","        self.return_all_layers = return_all_layers\n","\n","        cell_list = []\n","        for i in range(0, self.num_layers):\n","            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n","\n","            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n","                                          hidden_dim=self.hidden_dim[i],\n","                                          kernel_size=self.kernel_size[i],\n","                                          bias=self.bias))\n","\n","        self.cell_list = nn.ModuleList(cell_list)\n","\n","    def forward(self, input_tensor, hidden_state=None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        input_tensor: todo\n","            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n","        hidden_state: todo\n","            None. todo implement stateful\n","        Returns\n","        -------\n","        last_state_list, layer_output\n","        \"\"\"\n","        if not self.batch_first:\n","            # (t, b, c, h, w) -\u003e (b, t, c, h, w)\n","            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n","\n","        b, _, _, h, w = input_tensor.size()\n","\n","        # Implement stateful ConvLSTM\n","        if hidden_state is not None:\n","            raise NotImplementedError()\n","        else:\n","            # Since the init is done in forward. Can send image size here\n","            hidden_state = self._init_hidden(batch_size=b,\n","                                             image_size=(h, w))\n","\n","        layer_output_list = []\n","        last_state_list = []\n","\n","        seq_len = input_tensor.size(1)\n","        cur_layer_input = input_tensor\n","\n","        for layer_idx in range(self.num_layers):\n","\n","            h, c = hidden_state[layer_idx]\n","            output_inner = []\n","            for t in range(seq_len):\n","                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n","                                                 cur_state=[h, c])\n","                output_inner.append(h)\n","\n","            layer_output = T.stack(output_inner, dim=1)\n","            cur_layer_input = layer_output\n","\n","            layer_output_list.append(layer_output)\n","            last_state_list.append([h, c])\n","\n","        if not self.return_all_layers:\n","            layer_output_list = layer_output_list[-1:]\n","            last_state_list = last_state_list[-1:]\n","\n","        return layer_output_list, last_state_list\n","\n","    def _init_hidden(self, batch_size, image_size):\n","        init_states = []\n","        for i in range(self.num_layers):\n","            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n","        return init_states\n","\n","    @staticmethod\n","    def _check_kernel_size_consistency(kernel_size):\n","        if not (isinstance(kernel_size, tuple) or\n","                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n","            raise ValueError('`kernel_size` must be tuple or list of tuples')\n","\n","    @staticmethod\n","    def _extend_for_multilayer(param, num_layers):\n","        if not isinstance(param, list):\n","            param = [param] * num_layers\n","        return param"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1657518859260,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"GHXukHiF8U6q"},"outputs":[],"source":["\n","class Data(Dataset):\n","\n","  def __init__(self, samples, labels):\n","    self.samples = samples\n","    self.labels = labels\n","    self.len = samples.size(0)\n","  \n","  def __getitem__(self, index):\n","    return self.samples[index, :, :], self.labels[index, :]\n","\n","  def __len__(self):\n","    return self.len\n","\n","\n","class LazyData(Dataset):\n","\n","  def __init__(self, samples_path, labels_path):\n","    self.samples_path = samples_path\n","    self.labels_path = labels_path\n","    self.samples_list = [\n","      filename for filename in \n","      os.listdir(samples_path) if \n","      filename != '.DS_Store'\n","    ]\n","    labels_df = pd.read_csv(labels_path)\n","    self.labels_dict = {\n","      row['filename']: \n","      T.tensor([row['label']], dtype=T.float)\n","      for _, row in labels_df.iterrows()\n","    }\n","    self.len = len(self.samples_list)\n","  \n","  def __getitem__(self, index):\n","    filename = self.samples_list[index]\n","    sample_path = os.path.join(self.samples_path, filename)\n","    sample = T.load(sample_path)\n","    label = self.labels_dict[filename]\n","    return sample, label\n","\n","  def __len__(self):\n","    return self.len\n","\n","\n","class SmallData(Dataset):\n","\n","  def __init__(self, samples_path, labels_path, n_samples=1000):\n","    self.samples_path = samples_path\n","    self.labels_path = labels_path\n","    all_samples_names = sorted(\n","      os.listdir(samples_path),\n","      key=lambda x: int(os.path.splitext(x)[0])\n","    )\n","    all_labels = np.load(labels_path)\n","\n","    self.len = min(n_samples, len(all_samples_names))\n","    keep_indices = random.choices(\n","      range(self.len), \n","      k=n_samples\n","    )\n","\n","    samples_numpy = np.empty((n_samples, 15, 3, 224, 224), dtype=np.single)\n","    labels_numpy = np.empty((n_samples, 1), dtype=np.single)\n","    for i, k in enumerate(keep_indices):\n","      cur_sample_path = os.path.join(samples_path, all_samples_names[k])\n","      cur_sample = np.load(cur_sample_path)\n","      samples_numpy[i] = cur_sample\n","      labels_numpy[i] = all_labels[k]\n","\n","    self.samples = T.as_tensor(samples_numpy)\n","    self.labels = T.as_tensor(labels_numpy)\n","  \n","  def __getitem__(self, index):\n","    sample = self.samples[index]\n","    label = self.labels[index]\n","    return sample, label\n","\n","  def __len__(self):\n","    return self.len\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":441,"status":"ok","timestamp":1657518859697,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"UCe0i-8FjOyk"},"outputs":[],"source":["\n","def train_step(dataloader, device, model, criterion, optimizer, n_batches=1):\n","  optimizer.zero_grad()\n","  total_loss = 0\n","  len_counter = 0\n","  for k, (x, y) in enumerate(dataloader):\n","    x = x.to(device)\n","    y = y.to(device)\n","    pred = model(x)\n","    loss = criterion(pred, y)\n","    loss.backward()\n","    total_loss += x.size(0) * loss.item()\n","    len_counter += x.size(0)\n","    if k % n_batches == n_batches - 1:\n","      optimizer.step()\n","      optimizer.zero_grad()\n","  optimizer.step()\n","  optimizer.zero_grad()\n","  total_loss /= len_counter\n","  return total_loss\n","\n","\n","def eval_step(dataloader, device, model, criterion):\n","  total_loss = 0\n","  len_counter = 0\n","  with T.no_grad():\n","    for x, y in dataloader:\n","      x = x.to(device)\n","      y = y.to(device)\n","      pred = model(x)\n","      loss = criterion(pred, y)\n","      total_loss += x.size(0) * loss.item() \n","      len_counter += x.size(0)\n","  total_loss /= len_counter\n","  return total_loss\n","\n","\n","def train_loop(device, model, criterion, optimizer,\n","    train_loader, val_loader, test_loader,\n","    scheduler=None, init_epochs=100, add_epochs=False, verbose=False):\n","\n","  if verbose:\n","    print(model)\n","\n","  n_epochs = init_epochs\n","  total_epochs = init_epochs\n","  \n","  while n_epochs:\n","\n","    for epoch in range(n_epochs):\n","\n","      since = time.perf_counter()\n","\n","      model.train()\n","      train_loss = train_step(train_loader, device, model, criterion, optimizer, n_batches=4)\n","\n","      model.eval()\n","      val_loss = eval_step(val_loader, device, model, criterion)\n","\n","      if scheduler:\n","        scheduler.step(val_loss)\n","\n","      if verbose:\n","        print(\n","          f'{time.perf_counter() - since:\u003e6.1f}s :: ' +\n","          f'Epoch {total_epochs-n_epochs+epoch+1}/{total_epochs} complete.'\n","        )\n","        free_mem, total_mem = T.cuda.mem_get_info()\n","        print(\n","          ' ' * 11 +\n","          f'Device(free: {free_mem/(1024**2):.3f}, ' +\n","          f'total: {total_mem/(1024**2):.3f}).'\n","        )\n","        print(\n","          ' ' * 11 +\n","          f'Loss(train: {train_loss:.3f}, val: {val_loss:.3f}).'\n","        )\n","\n","    test_loss = eval_step(test_loader, device, model, criterion)\n","\n","    if verbose or add_epochs:\n","      print(f'Test loss: {test_loss}.')\n","    if add_epochs:\n","      n_epochs = input('Add epochs?')\n","      try:\n","        n_epochs = max(0, int(n_epochs))\n","        total_epochs += n_epochs\n","      except ValueError:\n","        n_epochs = 0\n","    else:\n","      n_epochs = 0\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1657518859697,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"ouYarQ8WDr5l"},"outputs":[],"source":["\n","class Model(nn.Module):\n","\n","  def __init__(self):\n","    super(Model, self).__init__()\n","\n","    self.clstm1 = ConvLSTM(\n","      input_dim=3,\n","      hidden_dim=48,\n","      num_layers=1,\n","      kernel_size=(7, 7),\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.pool1 = nn.MaxPool3d(\n","      kernel_size=(1, 3, 3),\n","      stride=(1, 3, 3)\n","    )\n","    self.norm1 = nn.InstanceNorm3d(\n","        num_features=48\n","    )\n","\n","    self.clstm2 = ConvLSTM(\n","      input_dim=48,\n","      hidden_dim=48,\n","      num_layers=1,\n","      kernel_size=(5, 5),\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.pool2 = nn.MaxPool3d(\n","      kernel_size=(1, 3, 3),\n","      stride=(1, 3, 3)\n","    )\n","    self.norm2 = nn.InstanceNorm3d(\n","        num_features=48\n","    )\n","\n","    self.clstm3 = ConvLSTM(\n","      input_dim=48,\n","      hidden_dim=48,\n","      num_layers=1,\n","      kernel_size=(5, 5),\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.pool3 = nn.MaxPool3d(\n","      kernel_size=(1, 3, 3),\n","      stride=(1, 3, 3)\n","    )\n","    self.norm3 = nn.InstanceNorm3d(\n","        num_features=48\n","    )\n","\n","    # dropout2d?\n","\n","    self.clstm4 = ConvLSTM(\n","      input_dim=48,\n","      hidden_dim=48,\n","      num_layers=1,\n","      kernel_size=(3, 3),\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.pool4 = nn.MaxPool2d(\n","      kernel_size=(2, 2),\n","      stride=(2, 2)\n","    )\n","    self.norm4 = nn.InstanceNorm2d(\n","        num_features=48\n","    )\n","\n","    # dropout2d?\n","\n","    self.fc1 = nn.Linear(in_features=768, out_features=1024)\n","    self.leakyrelu1 = nn.LeakyReLU()\n","    self.drop1 = nn.Dropout(p=0.5)  # 0.2 or 0.3\n","\n","    self.fc2 = nn.Linear(in_features=1024, out_features=1024)\n","    self.leakyrelu2 = nn.LeakyReLU()\n","    self.drop2 = nn.Dropout(p=0.5)\n","\n","    self.fc3 = nn.Linear(in_features=1024, out_features=512)\n","    self.leakyrelu3 = nn.LeakyReLU()\n","    self.drop3 = nn.Dropout(p=0.5)\n","    \n","    self.fc4 = nn.Linear(in_features=512, out_features=1)\n","  \n","  def forward(self, x):\n","\n","    x = self.clstm1(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.pool1(x)\n","    x = self.norm1(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = F.relu(x)\n","\n","    x = self.clstm2(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.pool2(x)\n","    x = self.norm2(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = F.relu(x)\n","\n","    x = self.clstm3(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.pool3(x)\n","    x = self.norm3(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = F.relu(x)\n","    \n","    x = self.clstm4(x)[0][0]\n","    x = x[:, -1, :, :, :]\n","    x = self.pool4(x)\n","    x = self.norm4(x)\n","    x = F.relu(x)\n","\n","    x = T.flatten(x, start_dim=1)\n","    x = self.fc1(x)\n","    x = self.leakyrelu1(x)\n","    x = self.drop1(x)\n","    x = self.fc2(x)\n","    x = self.leakyrelu2(x)\n","    x = self.drop2(x)\n","    x = self.fc3(x)\n","    x = self.leakyrelu3(x)\n","    x = self.drop3(x)\n","    x = self.fc4(x)\n","\n","    return x\n","  "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1168,"status":"ok","timestamp":1657518860863,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"BuqdzVA5xIvb","outputId":"bc25d663-c6fe-4b05-f8a2-79231218dd86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda.\n"]}],"source":["\n","device0 = T.device('cuda' if T.cuda.is_available() else 'cpu')\n","print(f'Using device: {device0}.')\n","\n","prop_val = 0.2\n","prop_test = 0.2\n","\n","full_dataset = LazyData(\n","  samples_path=os.path.join('data', 'samples'),\n","  labels_path=os.path.join('data', 'labels.csv')\n",")\n","len_val = int(prop_val * len(full_dataset))\n","len_test = int(prop_test * len(full_dataset))\n","len_train = len(full_dataset) - len_val - len_test\n","train, val, test = random_split(\n","  full_dataset, [len_train, len_val, len_test],\n","  generator=T.Generator().manual_seed(4)\n",")\n","\n","train_loader0 = DataLoader(train, batch_size=8, num_workers=2, shuffle=True)\n","val_loader0 = DataLoader(val, batch_size=8, num_workers=2)\n","test_loader0 = DataLoader(test, batch_size=8, num_workers=2)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1657518860864,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"_JJxRQlbPfw3"},"outputs":[],"source":["# # !unzip -q drive/MyDrive/data/model6.zip\n","# model0 = Model()\n","# model0.load_state_dict(T.load(\n","#   'model/model',\n","# ))\n","# model0 = model0.to(device0)\n","\n","# optimizer0 = T.optim.Adam(model0.parameters(), lr=0.0003)\n","# # optimizer0.load_state_dict(T.load(\n","# #   'model/optimizer'\n","# # ))\n","\n","# scheduler0 = T.optim.lr_scheduler.ReduceLROnPlateau(\n","#   optimizer0, factor=0.3, verbose=True, patience=10\n","# )\n","# # scheduler0.load_state_dict(T.load(\n","# #   'model/scheduler'\n","# # ))\n","\n","# criterion0 = nn.MSELoss()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":11685,"status":"ok","timestamp":1657518872546,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"},"user_tz":420},"id":"G9zdHgWfR49k"},"outputs":[],"source":["model0 = Model()\n","model0 = model0.to(device0)\n","\n","optimizer0 = T.optim.Adam(model0.parameters(), lr=0.001)\n","scheduler0 = T.optim.lr_scheduler.ReduceLROnPlateau(\n","  optimizer0, factor=0.3, verbose=True, patience=10\n",")\n","criterion0 = nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"C71rOS3I4aoP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model(\n","  (clstm1): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(51, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","      )\n","    )\n","  )\n","  (pool1): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3), padding=0, dilation=1, ceil_mode=False)\n","  (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","  (clstm2): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      )\n","    )\n","  )\n","  (pool2): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3), padding=0, dilation=1, ceil_mode=False)\n","  (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","  (clstm3): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      )\n","    )\n","  )\n","  (pool3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3), padding=0, dilation=1, ceil_mode=False)\n","  (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","  (clstm4): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (pool4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (norm4): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n","  (fc1): Linear(in_features=768, out_features=1024, bias=True)\n","  (leakyrelu1): LeakyReLU(negative_slope=0.01)\n","  (drop1): Dropout(p=0.5, inplace=False)\n","  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","  (leakyrelu2): LeakyReLU(negative_slope=0.01)\n","  (drop2): Dropout(p=0.5, inplace=False)\n","  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n","  (leakyrelu3): LeakyReLU(negative_slope=0.01)\n","  (drop3): Dropout(p=0.5, inplace=False)\n","  (fc4): Linear(in_features=512, out_features=1, bias=True)\n",")\n","2335.2s :: Epoch 1/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 39.259, val: 38.717).\n","2332.8s :: Epoch 2/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 29.569, val: 27.392).\n","2333.0s :: Epoch 3/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 24.613, val: 22.433).\n","2333.2s :: Epoch 4/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 21.680, val: 18.425).\n","2333.2s :: Epoch 5/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 19.922, val: 18.705).\n","2333.5s :: Epoch 6/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 18.620, val: 18.532).\n","2333.5s :: Epoch 7/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 17.768, val: 19.548).\n","2333.6s :: Epoch 8/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 16.601, val: 17.295).\n","2334.0s :: Epoch 9/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 15.984, val: 17.315).\n","2335.0s :: Epoch 10/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 14.238, val: 17.243).\n","2334.9s :: Epoch 11/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 13.647, val: 15.629).\n","2335.1s :: Epoch 12/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 15.594, val: 17.815).\n","2335.1s :: Epoch 13/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 16.851, val: 17.980).\n","2335.2s :: Epoch 14/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 14.487, val: 16.598).\n","2335.0s :: Epoch 15/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 14.992, val: 16.432).\n","2335.2s :: Epoch 16/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 14.321, val: 17.547).\n","2335.6s :: Epoch 17/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 13.149, val: 19.596).\n","2335.6s :: Epoch 18/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 12.489, val: 16.673).\n","2335.6s :: Epoch 19/201 complete.\n","           Device(free: 1225.750, total: 16280.875).\n","           Loss(train: 11.595, val: 19.280).\n"]}],"source":["train_loop(\n","  device0, model0, criterion0, optimizer0,\n","  train_loader0, val_loader0, test_loader0,\n","  scheduler=scheduler0, add_epochs=True, verbose=True,\n","  init_epochs=201\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8oQFFwYfCU-2"},"outputs":[],"source":["# !mkdir model\n","# T.save(model0.state_dict(), os.path.join('model', 'model'))\n","# T.save(optimizer0.state_dict(), os.path.join('model', 'optimizer'))\n","# T.save(scheduler0.state_dict(), os.path.join('model', 'scheduler'))\n","# !zip -r model6.zip model/\n","# !cp model6.zip drive/MyDrive/data/"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOP6BjCG3+kHo1rkCiQGtNy","collapsed_sections":[],"machine_shape":"hm","name":"baseline.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}