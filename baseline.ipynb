{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"baseline.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[],"authorship_tag":"ABX9TyN2ZrIcIDXbw5mKeIlVn6gj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"22IYkFG9GWNU","executionInfo":{"status":"ok","timestamp":1655574320449,"user_tz":420,"elapsed":3925,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"outputs":[],"source":["import os\n","import time\n","\n","import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from google.colab import drive\n","\n","import numpy as np"]},{"cell_type":"code","source":["# drive.mount('/content/drive/')\n","# !unzip -q drive/MyDrive/data/data2/samples.zip -d data\n","# !cp drive/MyDrive/data/data2/labels.npy data/"],"metadata":{"id":"b7R6_wUiWX86","executionInfo":{"status":"ok","timestamp":1655574792694,"user_tz":420,"elapsed":472248,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0fdc05ef-7eb2-49c5-a056-0362131ecae6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["# https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py\n","\n","\n","class ConvLSTMCell(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n","        \"\"\"\n","        Initialize ConvLSTM cell.\n","        Parameters\n","        ----------\n","        input_dim: int\n","            Number of channels of input tensor.\n","        hidden_dim: int\n","            Number of channels of hidden state.\n","        kernel_size: (int, int)\n","            Size of the convolutional kernel.\n","        bias: bool\n","            Whether or not to add the bias.\n","        \"\"\"\n","\n","        super(ConvLSTMCell, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.kernel_size = kernel_size\n","        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n","        self.bias = bias\n","\n","        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n","                              out_channels=4 * self.hidden_dim,\n","                              kernel_size=self.kernel_size,\n","                              padding=self.padding,\n","                              bias=self.bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        combined = T.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n","\n","        combined_conv = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = T.split(combined_conv, self.hidden_dim, dim=1)\n","        i = T.sigmoid(cc_i)\n","        f = T.sigmoid(cc_f)\n","        o = T.sigmoid(cc_o)\n","        g = T.tanh(cc_g)\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * T.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, image_size):\n","        height, width = image_size\n","        return (T.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n","                T.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n","\n","\n","class ConvLSTM(nn.Module):\n","\n","    \"\"\"\n","    Parameters:\n","        input_dim: Number of channels in input\n","        hidden_dim: Number of hidden channels\n","        kernel_size: Size of kernel in convolutions\n","        num_layers: Number of LSTM layers stacked on each other\n","        batch_first: Whether or not dimension 0 is the batch or not\n","        bias: Bias or no bias in Convolution\n","        return_all_layers: Return the list of computations for all layers\n","        Note: Will do same padding.\n","    Input:\n","        A tensor of size B, T, C, H, W or T, B, C, H, W\n","    Output:\n","        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n","            0 - layer_output_list is the list of lists of length T of each output\n","            1 - last_state_list is the list of last states\n","                    each element of the list is a tuple (h, c) for hidden state and memory\n","    Example:\n","        >> x = T.rand((32, 10, 64, 128, 128))\n","        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n","        >> _, last_states = convlstm(x)\n","        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n","    \"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n","                 batch_first=False, bias=True, return_all_layers=False):\n","        super(ConvLSTM, self).__init__()\n","\n","        self._check_kernel_size_consistency(kernel_size)\n","\n","        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n","        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n","        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n","        if not len(kernel_size) == len(hidden_dim) == num_layers:\n","            raise ValueError('Inconsistent list length.')\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.kernel_size = kernel_size\n","        self.num_layers = num_layers\n","        self.batch_first = batch_first\n","        self.bias = bias\n","        self.return_all_layers = return_all_layers\n","\n","        cell_list = []\n","        for i in range(0, self.num_layers):\n","            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n","\n","            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n","                                          hidden_dim=self.hidden_dim[i],\n","                                          kernel_size=self.kernel_size[i],\n","                                          bias=self.bias))\n","\n","        self.cell_list = nn.ModuleList(cell_list)\n","\n","    def forward(self, input_tensor, hidden_state=None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        input_tensor: todo\n","            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n","        hidden_state: todo\n","            None. todo implement stateful\n","        Returns\n","        -------\n","        last_state_list, layer_output\n","        \"\"\"\n","        if not self.batch_first:\n","            # (t, b, c, h, w) -> (b, t, c, h, w)\n","            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n","\n","        b, _, _, h, w = input_tensor.size()\n","\n","        # Implement stateful ConvLSTM\n","        if hidden_state is not None:\n","            raise NotImplementedError()\n","        else:\n","            # Since the init is done in forward. Can send image size here\n","            hidden_state = self._init_hidden(batch_size=b,\n","                                             image_size=(h, w))\n","\n","        layer_output_list = []\n","        last_state_list = []\n","\n","        seq_len = input_tensor.size(1)\n","        cur_layer_input = input_tensor\n","\n","        for layer_idx in range(self.num_layers):\n","\n","            h, c = hidden_state[layer_idx]\n","            output_inner = []\n","            for t in range(seq_len):\n","                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n","                                                 cur_state=[h, c])\n","                output_inner.append(h)\n","\n","            layer_output = T.stack(output_inner, dim=1)\n","            cur_layer_input = layer_output\n","\n","            layer_output_list.append(layer_output)\n","            last_state_list.append([h, c])\n","\n","        if not self.return_all_layers:\n","            layer_output_list = layer_output_list[-1:]\n","            last_state_list = last_state_list[-1:]\n","\n","        return layer_output_list, last_state_list\n","\n","    def _init_hidden(self, batch_size, image_size):\n","        init_states = []\n","        for i in range(self.num_layers):\n","            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n","        return init_states\n","\n","    @staticmethod\n","    def _check_kernel_size_consistency(kernel_size):\n","        if not (isinstance(kernel_size, tuple) or\n","                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n","            raise ValueError('`kernel_size` must be tuple or list of tuples')\n","\n","    @staticmethod\n","    def _extend_for_multilayer(param, num_layers):\n","        if not isinstance(param, list):\n","            param = [param] * num_layers\n","        return param"],"metadata":{"id":"qfxz0StraCEf","executionInfo":{"status":"ok","timestamp":1655574792956,"user_tz":420,"elapsed":264,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","class Data(Dataset):\n","\n","  def __init__(self, samples, labels):\n","    self.samples = samples\n","    self.labels = labels\n","    self.len = samples.size(0)\n","  \n","  def __getitem__(self, index):\n","    return self.samples[index, :, :], self.labels[index, :]\n","\n","  def __len__(self):\n","    return self.len\n","\n","\n","class LazyData(Dataset):\n","\n","  def __init__(self, samples_path, labels_path):\n","    self.samples_path = samples_path\n","    self.labels_path = labels_path\n","    self.samples_list = sorted(\n","        os.listdir(samples_path),\n","        key=lambda x: int(os.path.splitext(x)[0])\n","    )\n","    labels = np.load(labels_path)\n","    self.labels = T.tensor(labels)\n","    self.len = len(self.samples_list)\n","  \n","  def __getitem__(self, index):\n","    sample_path = os.path.join(self.samples_path, self.samples_list[index])\n","    sample = np.load(sample_path)\n","    sample = T.from_numpy(sample)\n","    label = self.labels[index, :]\n","    return sample, label\n","\n","  def __len__(self):\n","    return self.len\n","\n"],"metadata":{"id":"GHXukHiF8U6q","executionInfo":{"status":"ok","timestamp":1655574792956,"user_tz":420,"elapsed":3,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def pram():\n","  a, b = T.cuda.mem_get_info()\n","  print(f'{a/(1024**2):.2f}MB free of {b/(1024**2):.2f}MB')\n","\n","class Model(nn.Module):\n","\n","  def __init__(self):\n","    super(Model, self).__init__()\n","\n","    dim1 = 32\n","    self.clstm1 = ConvLSTM(\n","      input_dim=3,\n","      hidden_dim=dim1,\n","      kernel_size=(5, 5),\n","      num_layers=1,\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.batchnorm1 = nn.BatchNorm3d(dim1)\n","    self.maxpool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n","\n","    dim2 = 32\n","    self.clstm2 = ConvLSTM(\n","      input_dim=dim1,\n","      hidden_dim=dim2,\n","      kernel_size=(5, 5),\n","      num_layers=1,\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.batchnorm2 = nn.BatchNorm3d(dim2)\n","    self.maxpool2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n","\n","    dim3 = 24\n","    self.clstm3 = ConvLSTM(\n","      input_dim=dim2,\n","      hidden_dim=dim3,\n","      kernel_size=(3, 3),\n","      num_layers=1,\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.batchnorm3 = nn.BatchNorm3d(dim3)\n","    self.maxpool3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n","\n","    dim4 = 24\n","    self.clstm4 = ConvLSTM(\n","      input_dim=dim3,\n","      hidden_dim=dim4,\n","      kernel_size=(3, 3),\n","      num_layers=1,\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.batchnorm4 = nn.BatchNorm3d(dim4)\n","    self.maxpool4 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n","\n","    dim5 = 16\n","    self.clstm5 = ConvLSTM(\n","      input_dim=dim4,\n","      hidden_dim=dim5,\n","      kernel_size=(3, 3),\n","      num_layers=1,\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.batchnorm5 = nn.BatchNorm2d(dim5)\n","    self.maxpool5 = nn.MaxPool2d(kernel_size=(2, 2))\n","    \n","    self.fc1 = nn.Linear(in_features=784, out_features=512)\n","    self.fc2 = nn.Linear(in_features=512, out_features=512)\n","    self.fc3 = nn.Linear(in_features=512, out_features=256)\n","    self.fc4 = nn.Linear(in_features=256, out_features=1)\n","  \n","  def forward(self, x):\n","\n","    x = self.clstm1(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.batchnorm1(x)\n","    x = self.maxpool1(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","\n","    x = self.clstm2(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.batchnorm2(x)\n","    x = self.maxpool2(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","\n","    x = self.clstm3(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.batchnorm3(x)\n","    x = self.maxpool3(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","\n","    x = self.clstm4(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.batchnorm4(x)\n","    x = self.maxpool4(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","\n","    x = self.clstm5(x)[0][0]\n","    x = x[:, -1, :, :, :]\n","    x = self.batchnorm5(x)\n","    x = self.maxpool5(x)\n","\n","    x = T.flatten(x, start_dim=1)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","    x = F.relu(x)\n","    x = self.fc4(x)\n","\n","    return x\n","  "],"metadata":{"id":"ouYarQ8WDr5l","executionInfo":{"status":"ok","timestamp":1655574792957,"user_tz":420,"elapsed":3,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","def train_step(dataloader, device, model, criterion, optimizer):\n","  total_loss = 0\n","  len_counter = 0\n","  for x, y in dataloader:\n","    x = x.to(device)\n","    y = y.to(device)\n","    optimizer.zero_grad()\n","    pred = model(x)\n","    loss = criterion(pred, y)\n","    loss.backward()\n","    optimizer.step()\n","    total_loss += x.size(0) * loss.item()\n","    len_counter += x.size(0)\n","  total_loss /= len_counter\n","  return total_loss\n","\n","\n","def eval_step(dataloader, device, model, criterion):\n","  total_loss = 0\n","  len_counter = 0\n","  with T.no_grad():\n","    for x, y in dataloader:\n","      x = x.to(device)\n","      y = y.to(device)\n","      pred = model(x)\n","      loss = criterion(pred, y)\n","      total_loss += x.size(0) * loss.item() \n","      len_counter += x.size(0)\n","  total_loss /= len_counter\n","  return total_loss\n","\n","\n","def train_loop(device, model, criterion, optimizer,\n","    train_loader, val_loader, test_loader,\n","    scheduler=None, init_epochs=100, add_epochs=False, verbose=False):\n","\n","  model = model.to(device)\n","  if verbose:\n","    print(model)\n","\n","  n_epochs = init_epochs\n","  total_epochs = init_epochs\n","  \n","  while n_epochs:\n","\n","    for epoch in range(n_epochs):\n","\n","      since = time.perf_counter()\n","\n","      model.train()\n","      train_loss = train_step(train_loader, device, model, criterion, optimizer)\n","\n","      model.eval()\n","      val_loss = eval_step(val_loader, device, model, criterion)\n","\n","      if scheduler:\n","        scheduler.step(val_loss)\n","\n","      if verbose:\n","        print(\n","          f'{time.perf_counter() - since:>5.1f}s :: ' +\n","          f'Epoch {total_epochs-n_epochs+epoch+1}/{total_epochs} complete.'\n","        )\n","        free_mem, total_mem = T.cuda.mem_get_info()\n","        print(\n","          ' ' * 10 +\n","          f'Device(free: {free_mem/(1024**2):.3f}, ' +\n","          f'total: {total_mem/(1024**2):.3f}).'\n","        )\n","        print(\n","          ' ' * 10 +\n","          f'Loss(train: {train_loss:.3f}, val: {val_loss:.3f}).'\n","        )\n","\n","    test_loss = eval_step(test_loader, device, model, criterion)\n","\n","    if verbose or add_epochs:\n","      print(f'Test loss: {test_loss}.')\n","    if add_epochs:\n","      n_epochs = input('Add epochs?')\n","      try:\n","        n_epochs = max(0, int(n_epochs))\n","        total_epochs += n_epochs\n","      except ValueError:\n","        n_epochs = 0\n","    else:\n","      n_epochs = 0\n","    \n","    T.save(model.state_dict(), 'model')\n","    files.download('model')\n"],"metadata":{"id":"UCe0i-8FjOyk","executionInfo":{"status":"ok","timestamp":1655574793204,"user_tz":420,"elapsed":250,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","device0 = T.device('cuda' if T.cuda.is_available() else 'cpu')\n","print(f'Using device: {device0}.')\n","\n","prop_val = 0.2\n","prop_test = 0.2\n","\n","full_dataset = LazyData(\n","  os.path.join('data', 'data2'),\n","  os.path.join('data', 'labels.npy')\n",")\n","len_val = int(prop_val * len(full_dataset))\n","len_test = int(prop_test * len(full_dataset))\n","len_train = len(full_dataset) - len_val - len_test\n","train, val, test = random_split(\n","  full_dataset, [len_train, len_val, len_test],\n","  generator=T.Generator().manual_seed(4)\n",")\n"],"metadata":{"id":"BuqdzVA5xIvb","executionInfo":{"status":"ok","timestamp":1655574793499,"user_tz":420,"elapsed":4,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"68c0190c-27e4-4865-f1b2-7ba7d320cc12"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda.\n"]}]},{"cell_type":"code","source":["\n","train_loader0 = DataLoader(train, batch_size=8, shuffle=True, num_workers=8)\n","val_loader0 = DataLoader(val, batch_size=8, num_workers=8)\n","test_loader0 = DataLoader(test, batch_size=8, num_workers=8)\n","\n","model0 = Model()\n","\n","criterion0 = nn.MSELoss()\n","optimizer0 = T.optim.Adam(model0.parameters(), 0.01)\n","scheduler0 = T.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer0, factor=0.3, verbose=True, patience=10\n",")\n","\n","train_loop(\n","  device0, model0, criterion0, optimizer0,\n","  train_loader0, val_loader0, test_loader0,\n","  scheduler=scheduler0, add_epochs=True, verbose=True,\n","  init_epochs=201\n",")"],"metadata":{"id":"C71rOS3I4aoP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5d9c9d4-339d-470d-b818-b35d674020e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Model(\n","  (clstm1): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(35, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      )\n","    )\n","  )\n","  (batchnorm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (maxpool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (clstm2): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      )\n","    )\n","  )\n","  (batchnorm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (maxpool2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (clstm3): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(56, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (batchnorm3): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (maxpool3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (clstm4): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (batchnorm4): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (maxpool4): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (clstm5): ConvLSTM(\n","    (cell_list): ModuleList(\n","      (0): ConvLSTMCell(\n","        (conv): Conv2d(40, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (batchnorm5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (maxpool5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=784, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=512, bias=True)\n","  (fc3): Linear(in_features=512, out_features=256, bias=True)\n","  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",")\n","569.1s :: Epoch 1/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 45.788, val: 35.317).\n","565.1s :: Epoch 2/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 38.041, val: 41.248).\n","566.7s :: Epoch 3/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 39.996, val: 44.224).\n","568.3s :: Epoch 4/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 39.920, val: 42.887).\n","566.2s :: Epoch 5/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 39.399, val: 39.798).\n","567.2s :: Epoch 6/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 38.681, val: 39.139).\n","567.0s :: Epoch 7/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 39.196, val: 43.506).\n","566.6s :: Epoch 8/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 39.228, val: 40.695).\n","566.5s :: Epoch 9/201 complete.\n","          Device(free: 3317.750, total: 16280.875).\n","          Loss(train: 39.102, val: 38.242).\n"]}]}]}