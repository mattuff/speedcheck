{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"baseline.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[],"authorship_tag":"ABX9TyMVAGFtqEJ0oL9yDApbfcji"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"22IYkFG9GWNU","executionInfo":{"status":"ok","timestamp":1655719528413,"user_tz":420,"elapsed":2532,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"outputs":[],"source":["import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","from google.colab import drive\n","\n","import numpy as np\n","\n","import os\n","import time\n","import random"]},{"cell_type":"code","source":["# drive.mount('/content/drive/')\n","# !unzip -q drive/MyDrive/data/data2/samples.zip -d data\n","# !cp drive/MyDrive/data/data2/labels.npy data/"],"metadata":{"id":"b7R6_wUiWX86","executionInfo":{"status":"ok","timestamp":1655720206454,"user_tz":420,"elapsed":678044,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"87f5151f-dc31-4269-cacf-e3cec2a0f395"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["# https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py\n","\n","\n","class ConvLSTMCell(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n","        \"\"\"\n","        Initialize ConvLSTM cell.\n","        Parameters\n","        ----------\n","        input_dim: int\n","            Number of channels of input tensor.\n","        hidden_dim: int\n","            Number of channels of hidden state.\n","        kernel_size: (int, int)\n","            Size of the convolutional kernel.\n","        bias: bool\n","            Whether or not to add the bias.\n","        \"\"\"\n","\n","        super(ConvLSTMCell, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.kernel_size = kernel_size\n","        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n","        self.bias = bias\n","\n","        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n","                              out_channels=4 * self.hidden_dim,\n","                              kernel_size=self.kernel_size,\n","                              padding=self.padding,\n","                              bias=self.bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        combined = T.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n","\n","        combined_conv = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = T.split(combined_conv, self.hidden_dim, dim=1)\n","        i = T.sigmoid(cc_i)\n","        f = T.sigmoid(cc_f)\n","        o = T.sigmoid(cc_o)\n","        g = T.tanh(cc_g)\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * T.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, image_size):\n","        height, width = image_size\n","        return (T.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n","                T.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n","\n","\n","class ConvLSTM(nn.Module):\n","\n","    \"\"\"\n","    Parameters:\n","        input_dim: Number of channels in input\n","        hidden_dim: Number of hidden channels\n","        kernel_size: Size of kernel in convolutions\n","        num_layers: Number of LSTM layers stacked on each other\n","        batch_first: Whether or not dimension 0 is the batch or not\n","        bias: Bias or no bias in Convolution\n","        return_all_layers: Return the list of computations for all layers\n","        Note: Will do same padding.\n","    Input:\n","        A tensor of size B, T, C, H, W or T, B, C, H, W\n","    Output:\n","        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n","            0 - layer_output_list is the list of lists of length T of each output\n","            1 - last_state_list is the list of last states\n","                    each element of the list is a tuple (h, c) for hidden state and memory\n","    Example:\n","        >> x = T.rand((32, 10, 64, 128, 128))\n","        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n","        >> _, last_states = convlstm(x)\n","        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n","    \"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n","                 batch_first=False, bias=True, return_all_layers=False):\n","        super(ConvLSTM, self).__init__()\n","\n","        self._check_kernel_size_consistency(kernel_size)\n","\n","        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n","        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n","        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n","        if not len(kernel_size) == len(hidden_dim) == num_layers:\n","            raise ValueError('Inconsistent list length.')\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.kernel_size = kernel_size\n","        self.num_layers = num_layers\n","        self.batch_first = batch_first\n","        self.bias = bias\n","        self.return_all_layers = return_all_layers\n","\n","        cell_list = []\n","        for i in range(0, self.num_layers):\n","            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n","\n","            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n","                                          hidden_dim=self.hidden_dim[i],\n","                                          kernel_size=self.kernel_size[i],\n","                                          bias=self.bias))\n","\n","        self.cell_list = nn.ModuleList(cell_list)\n","\n","    def forward(self, input_tensor, hidden_state=None):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        input_tensor: todo\n","            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n","        hidden_state: todo\n","            None. todo implement stateful\n","        Returns\n","        -------\n","        last_state_list, layer_output\n","        \"\"\"\n","        if not self.batch_first:\n","            # (t, b, c, h, w) -> (b, t, c, h, w)\n","            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n","\n","        b, _, _, h, w = input_tensor.size()\n","\n","        # Implement stateful ConvLSTM\n","        if hidden_state is not None:\n","            raise NotImplementedError()\n","        else:\n","            # Since the init is done in forward. Can send image size here\n","            hidden_state = self._init_hidden(batch_size=b,\n","                                             image_size=(h, w))\n","\n","        layer_output_list = []\n","        last_state_list = []\n","\n","        seq_len = input_tensor.size(1)\n","        cur_layer_input = input_tensor\n","\n","        for layer_idx in range(self.num_layers):\n","\n","            h, c = hidden_state[layer_idx]\n","            output_inner = []\n","            for t in range(seq_len):\n","                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n","                                                 cur_state=[h, c])\n","                output_inner.append(h)\n","\n","            layer_output = T.stack(output_inner, dim=1)\n","            cur_layer_input = layer_output\n","\n","            layer_output_list.append(layer_output)\n","            last_state_list.append([h, c])\n","\n","        if not self.return_all_layers:\n","            layer_output_list = layer_output_list[-1:]\n","            last_state_list = last_state_list[-1:]\n","\n","        return layer_output_list, last_state_list\n","\n","    def _init_hidden(self, batch_size, image_size):\n","        init_states = []\n","        for i in range(self.num_layers):\n","            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n","        return init_states\n","\n","    @staticmethod\n","    def _check_kernel_size_consistency(kernel_size):\n","        if not (isinstance(kernel_size, tuple) or\n","                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n","            raise ValueError('`kernel_size` must be tuple or list of tuples')\n","\n","    @staticmethod\n","    def _extend_for_multilayer(param, num_layers):\n","        if not isinstance(param, list):\n","            param = [param] * num_layers\n","        return param"],"metadata":{"id":"qfxz0StraCEf","executionInfo":{"status":"ok","timestamp":1655720206861,"user_tz":420,"elapsed":410,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","class Data(Dataset):\n","\n","  def __init__(self, samples, labels):\n","    self.samples = samples\n","    self.labels = labels\n","    self.len = samples.size(0)\n","  \n","  def __getitem__(self, index):\n","    return self.samples[index, :, :], self.labels[index, :]\n","\n","  def __len__(self):\n","    return self.len\n","\n","\n","class LazyData(Dataset):\n","\n","  def __init__(self, samples_path, labels_path):\n","    self.samples_path = samples_path\n","    self.labels_path = labels_path\n","    self.samples_list = sorted(\n","      os.listdir(samples_path),\n","      key=lambda x: int(os.path.splitext(x)[0])\n","    )\n","    labels = np.load(labels_path)\n","    self.labels = T.tensor(labels)\n","    self.len = len(self.samples_list)\n","  \n","  def __getitem__(self, index):\n","    sample_path = os.path.join(self.samples_path, self.samples_list[index])\n","    sample = np.load(sample_path)\n","    sample = T.from_numpy(sample)\n","    label = self.labels[index, :]\n","    return sample, label\n","\n","  def __len__(self):\n","    return self.len\n","\n","\n","class SmallData(Dataset):\n","\n","  def __init__(self, samples_path, labels_path, n_samples=1):\n","    self.samples_path = samples_path\n","    self.labels_path = labels_path\n","    all_samples_names = sorted(\n","      os.listdir(samples_path),\n","      key=lambda x: int(os.path.splitext(x)[0])\n","    )\n","    all_labels = np.load(labels_path)\n","\n","    self.len = min(n_samples, len(all_samples_names))\n","    keep_indices = random.choices(\n","      range(self.len), \n","      k=n_samples\n","    )\n","\n","    samples_numpy = np.empty((n_samples, 15, 3, 224, 224), dtype=np.single)\n","    labels_numpy = np.empty((n_samples, 1), dtype=np.single)\n","    for i, k in enumerate(keep_indices):\n","      cur_sample_path = os.path.join(samples_path, all_samples_names[k])\n","      cur_sample = np.load(cur_sample_path)\n","      samples_numpy[i] = cur_sample\n","      labels_numpy[i] = all_labels[k]\n","\n","    self.samples = T.as_tensor(samples_numpy)\n","    self.labels = T.as_tensor(labels_numpy)\n","  \n","  def __getitem__(self, index):\n","    sample = self.samples[index]\n","    label = self.labels[index]\n","    return sample, label\n","\n","  def __len__(self):\n","    return self.len\n","\n"],"metadata":{"id":"GHXukHiF8U6q","executionInfo":{"status":"ok","timestamp":1655720206861,"user_tz":420,"elapsed":4,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def pram():\n","  free, total = T.cuda.mem_get_info()\n","  print(f'{free/1024**2:.2f} free of {total/1024**2:.2f}.')\n","\n","\n","class ConvLSTMBlock(nn.Module):\n","\n","  def __init__(self, input_dim, hidden_dim, num_layers,\n","    conv_kernel_size=(3, 3), pool_kernel_size=(1, 2, 2)):\n","    \n","    super(ConvLSTMBlock, self).__init__()\n","    self.convlstm = ConvLSTM(\n","      input_dim=input_dim,\n","      hidden_dim=hidden_dim,\n","      kernel_size=conv_kernel_size,\n","      num_layers=num_layers,\n","      bias=False,\n","      batch_first=True\n","    )\n","    self.avgpool = nn.AvgPool3d(kernel_size=pool_kernel_size)\n","  \n","  def forward(self, x):\n","    x = self.convlstm(x)[0][0]\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    x = self.avgpool(x)\n","    x = T.permute(x, (0, 2, 1, 3, 4))\n","    return x\n","\n","\n","class Model(nn.Module):\n","\n","  def __init__(self):\n","    super(Model, self).__init__()\n","\n","    self.clstm1 = ConvLSTMBlock(\n","      input_dim=3,\n","      hidden_dim=24,\n","      num_layers=1,\n","      conv_kernel_size=(5, 5),\n","      pool_kernel_size=(1, 4, 4)\n","    )\n","\n","    self.clstm2 = ConvLSTMBlock(\n","      input_dim=24,\n","      hidden_dim=48,\n","      num_layers=1,\n","      conv_kernel_size=(5, 5),\n","      pool_kernel_size=(1, 3, 3)\n","    )\n","\n","    self.clstm3 = ConvLSTMBlock(\n","      input_dim=48,\n","      hidden_dim=128,\n","      num_layers=1,\n","      conv_kernel_size=(3, 3),\n","      pool_kernel_size=(1, 3, 3)\n","    )\n","\n","    self.clstm4 = ConvLSTMBlock(\n","      input_dim=128,\n","      hidden_dim=256,\n","      num_layers=1,\n","      conv_kernel_size=(3, 3),\n","      pool_kernel_size=(1, 2, 2)\n","    )\n","\n","    self.conv = nn.Conv2d(\n","      in_channels=256, \n","      out_channels=1024,\n","      kernel_size=(3, 3)\n","    )\n","    \n","    self.fc1 = nn.Linear(in_features=1024, out_features=1024)\n","    self.fc2 = nn.Linear(in_features=1024, out_features=512)\n","    self.fc3 = nn.Linear(in_features=512, out_features=256)\n","    self.fc4 = nn.Linear(in_features=256, out_features=1)\n","  \n","  def forward(self, x):\n","\n","    x = self.clstm1(x)\n","    x = F.relu(x)\n","    x = self.clstm2(x)\n","    x = F.relu(x)\n","    x = self.clstm3(x)\n","    x = F.relu(x)\n","    x = self.clstm4(x)\n","\n","    x = x[:, -1, :, :, :]\n","    x = self.conv(x)\n","\n","    x = T.flatten(x, start_dim=1)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","    x = F.relu(x)\n","    x = self.fc4(x)\n","\n","    return x\n","  "],"metadata":{"id":"ouYarQ8WDr5l","executionInfo":{"status":"ok","timestamp":1655720206862,"user_tz":420,"elapsed":4,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","def train_step(dataloader, device, model, criterion, optimizer):\n","  total_loss = 0\n","  len_counter = 0\n","  for x, y in dataloader:\n","    x = x.to(device)\n","    y = y.to(device)\n","    optimizer.zero_grad()\n","    pred = model(x)\n","    loss = criterion(pred, y)\n","    loss.backward()\n","    optimizer.step()\n","    total_loss += x.size(0) * loss.item()\n","    len_counter += x.size(0)\n","  total_loss /= len_counter\n","  return total_loss\n","\n","\n","def eval_step(dataloader, device, model, criterion):\n","  total_loss = 0\n","  len_counter = 0\n","  with T.no_grad():\n","    for x, y in dataloader:\n","      x = x.to(device)\n","      y = y.to(device)\n","      pred = model(x)\n","      loss = criterion(pred, y)\n","      total_loss += x.size(0) * loss.item() \n","      len_counter += x.size(0)\n","  total_loss /= len_counter\n","  return total_loss\n","\n","\n","def train_loop(device, model, criterion, optimizer,\n","    train_loader, val_loader, test_loader,\n","    scheduler=None, init_epochs=100, add_epochs=False, verbose=False):\n","\n","  model = model.to(device)\n","  if verbose:\n","    print(model)\n","\n","  n_epochs = init_epochs\n","  total_epochs = init_epochs\n","  \n","  while n_epochs:\n","\n","    for epoch in range(n_epochs):\n","\n","      since = time.perf_counter()\n","\n","      model.train()\n","      train_loss = train_step(train_loader, device, model, criterion, optimizer)\n","\n","      model.eval()\n","      val_loss = eval_step(val_loader, device, model, criterion)\n","\n","      if scheduler:\n","        scheduler.step(val_loss)\n","\n","      if verbose:\n","        print(\n","          f'{time.perf_counter() - since:>5.1f}s :: ' +\n","          f'Epoch {total_epochs-n_epochs+epoch+1}/{total_epochs} complete.'\n","        )\n","        free_mem, total_mem = T.cuda.mem_get_info()\n","        print(\n","          ' ' * 10 +\n","          f'Device(free: {free_mem/(1024**2):.3f}, ' +\n","          f'total: {total_mem/(1024**2):.3f}).'\n","        )\n","        print(\n","          ' ' * 10 +\n","          f'Loss(train: {train_loss:.3f}, val: {val_loss:.3f}).'\n","        )\n","\n","    test_loss = eval_step(test_loader, device, model, criterion)\n","\n","    if verbose or add_epochs:\n","      print(f'Test loss: {test_loss}.')\n","    if add_epochs:\n","      n_epochs = input('Add epochs?')\n","      try:\n","        n_epochs = max(0, int(n_epochs))\n","        total_epochs += n_epochs\n","      except ValueError:\n","        n_epochs = 0\n","    else:\n","      n_epochs = 0\n"],"metadata":{"id":"UCe0i-8FjOyk","executionInfo":{"status":"ok","timestamp":1655720209768,"user_tz":420,"elapsed":2910,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","device0 = T.device('cuda' if T.cuda.is_available() else 'cpu')\n","print(f'Using device: {device0}.')\n","\n","prop_val = 0.2\n","prop_test = 0.2\n","\n","full_dataset = LazyData(\n","  os.path.join('data', 'data2'),\n","  os.path.join('data', 'labels.npy')\n",")\n","len_val = int(prop_val * len(full_dataset))\n","len_test = int(prop_test * len(full_dataset))\n","len_train = len(full_dataset) - len_val - len_test\n","train, val, test = random_split(\n","  full_dataset, [len_train, len_val, len_test],\n","  generator=T.Generator().manual_seed(4)\n",")\n"],"metadata":{"id":"BuqdzVA5xIvb","executionInfo":{"status":"ok","timestamp":1655720209999,"user_tz":420,"elapsed":12,"user":{"displayName":"Matt Uffenheimer","userId":"08273244796095023606"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fdf82717-2caa-4b6e-e4f9-946fc95925b2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda.\n"]}]},{"cell_type":"code","source":["\n","train_loader0 = DataLoader(train, batch_size=8, num_workers=4, shuffle=True)\n","val_loader0 = DataLoader(val, batch_size=8, num_workers=4)\n","test_loader0 = DataLoader(test, batch_size=8, num_workers=4)\n","\n","model0 = Model()\n","\n","criterion0 = nn.MSELoss()\n","optimizer0 = T.optim.Adam(model0.parameters(), lr=0.01)\n","scheduler0 = T.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer0, factor=0.3, verbose=True, patience=10\n",")\n","\n","train_loop(\n","  device0, model0, criterion0, optimizer0,\n","  train_loader0, val_loader0, test_loader0,\n","  scheduler=scheduler0, add_epochs=True, verbose=True,\n","  init_epochs=201\n",")"],"metadata":{"id":"C71rOS3I4aoP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27a007b3-d44f-40d7-88f3-35ae64a81705"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model(\n","  (clstm1): ConvLSTMBlock(\n","    (convlstm): ConvLSTM(\n","      (cell_list): ModuleList(\n","        (0): ConvLSTMCell(\n","          (conv): Conv2d(27, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","        )\n","      )\n","    )\n","    (avgpool): AvgPool3d(kernel_size=(1, 4, 4), stride=(1, 4, 4), padding=0)\n","  )\n","  (clstm2): ConvLSTMBlock(\n","    (convlstm): ConvLSTM(\n","      (cell_list): ModuleList(\n","        (0): ConvLSTMCell(\n","          (conv): Conv2d(72, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","        )\n","      )\n","    )\n","    (avgpool): AvgPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3), padding=0)\n","  )\n","  (clstm3): ConvLSTMBlock(\n","    (convlstm): ConvLSTM(\n","      (cell_list): ModuleList(\n","        (0): ConvLSTMCell(\n","          (conv): Conv2d(176, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","    (avgpool): AvgPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3), padding=0)\n","  )\n","  (clstm4): ConvLSTMBlock(\n","    (convlstm): ConvLSTM(\n","      (cell_list): ModuleList(\n","        (0): ConvLSTMCell(\n","          (conv): Conv2d(384, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","    (avgpool): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)\n","  )\n","  (conv): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n","  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n","  (fc3): Linear(in_features=512, out_features=256, bias=True)\n","  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",")\n","424.0s :: Epoch 1/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 621.349, val: 41.033).\n","419.4s :: Epoch 2/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 41.758, val: 41.167).\n","420.2s :: Epoch 3/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 41.151, val: 39.912).\n","452.9s :: Epoch 4/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 41.807, val: 44.528).\n","519.0s :: Epoch 5/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 43.715, val: 40.658).\n","518.5s :: Epoch 6/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.508, val: 39.947).\n","518.3s :: Epoch 7/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.466, val: 40.197).\n","518.1s :: Epoch 8/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.416, val: 39.848).\n","518.0s :: Epoch 9/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.457, val: 39.818).\n","517.9s :: Epoch 10/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.418, val: 39.857).\n","517.8s :: Epoch 11/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.399, val: 39.868).\n","517.7s :: Epoch 12/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.366, val: 39.839).\n","518.1s :: Epoch 13/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.413, val: 40.036).\n","517.2s :: Epoch 14/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.447, val: 39.821).\n","516.8s :: Epoch 15/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.356, val: 39.817).\n","517.3s :: Epoch 16/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.324, val: 40.206).\n","517.3s :: Epoch 17/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.416, val: 39.992).\n","517.2s :: Epoch 18/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.371, val: 39.920).\n","516.9s :: Epoch 19/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.391, val: 39.823).\n","Epoch 00020: reducing learning rate of group 0 to 3.0000e-03.\n","517.2s :: Epoch 20/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.355, val: 39.858).\n","517.2s :: Epoch 21/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.270, val: 39.820).\n","517.0s :: Epoch 22/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.250, val: 39.854).\n","515.8s :: Epoch 23/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.332, val: 39.817).\n","517.2s :: Epoch 24/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.288, val: 39.835).\n","516.7s :: Epoch 25/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.267, val: 39.844).\n","516.7s :: Epoch 26/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.285, val: 39.821).\n","515.6s :: Epoch 27/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.270, val: 39.818).\n","517.0s :: Epoch 28/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.296, val: 39.819).\n","516.6s :: Epoch 29/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.268, val: 39.817).\n","515.4s :: Epoch 30/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.283, val: 39.831).\n","Epoch 00031: reducing learning rate of group 0 to 9.0000e-04.\n","516.3s :: Epoch 31/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.239, val: 39.872).\n","516.5s :: Epoch 32/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.244, val: 39.817).\n","515.2s :: Epoch 33/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.242, val: 39.817).\n","516.4s :: Epoch 34/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.250, val: 39.817).\n","516.6s :: Epoch 35/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.244, val: 39.817).\n","515.1s :: Epoch 36/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.241, val: 39.817).\n","516.5s :: Epoch 37/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.242, val: 39.819).\n","514.7s :: Epoch 38/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.253, val: 39.827).\n","449.1s :: Epoch 39/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.244, val: 39.818).\n","422.1s :: Epoch 40/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.238, val: 39.820).\n","422.9s :: Epoch 41/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.244, val: 39.817).\n","Epoch 00042: reducing learning rate of group 0 to 2.7000e-04.\n","423.5s :: Epoch 42/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.240, val: 39.818).\n","507.0s :: Epoch 43/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.232, val: 39.817).\n","514.8s :: Epoch 44/201 complete.\n","          Device(free: 7745.750, total: 16280.875).\n","          Loss(train: 39.230, val: 39.817).\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ajowc6HjeWo_"},"execution_count":null,"outputs":[]}]}